{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project creates and updates a Google Sheet with relevant tweets and a basic sentiment analysis of this tweet. \n",
    "\n",
    "**The problem**\n",
    "I was monitoring a few companies in the context of a larger project and needed to get an update of their activity and what the public thought about them. While I was reading the news and checking sometimes social media, I needed to find a more efficient way to see and analysis what was going on. \n",
    "\n",
    "**The solution** \n",
    "I created several pieces of code to analysis online newspapers and social media. This particular code here scraps Twitter and returns structured information about the latest tweets. \n",
    "\n",
    "**The risks**  \n",
    "- I couldn't scrap tweets that were older than 2 weeks \n",
    "- the keywords I used were basics and therefore, combined with the noise on the social platform, I could not trust 100% the results\n",
    "\n",
    "**The tools**\n",
    "\n",
    "I used several libraries: \n",
    "\n",
    "1. (Aylien)[aylien.com] \n",
    "They are a company specialised in NLP product. They provide an API with a limit of calls per day. Their NLP library gave me a more precise results than the TwitterSearch Library as well as a percentage of confidence. \n",
    "\n",
    "2. Google Sheet  \n",
    "I used the Google Sheet API because I am a big fan of observing my results directly in an Excel when I can to give me a clear results and also to share them quickly with others. This API set up has changed a little bit since I have written this code but the concept is the same. More information can be found (here)[https://developers.google.com/sheets/api/] \n",
    "\n",
    "3. TwitterSearch\n",
    "There is a lot of API wrapper for Twitte out there but I like this one in particular because of the clarity of the documentation and the functions provided. (This library)[https://pypi.org/project/TwitterSearch/] has been created by the Technical University of Munich. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from TwitterSearch import *\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gspread \n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from aylienapiclient import textapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "\n",
    "\n",
    "client_aylien = textapi.Client(\"******\", \"********************\")\n",
    "\n",
    "\n",
    "\n",
    "ts = TwitterSearch(\n",
    "                consumer_key = '**********',\n",
    "                consumer_secret = '**************',\n",
    "                access_token = '****************',\n",
    "                access_token_secret = '***************',\n",
    "                tweet_mode = 'extended'\n",
    "                )\n",
    "\n",
    "\n",
    "#Google Sheet API \n",
    "\n",
    "client_secret = r'C:\\Users\\Ethel Karskens\\Projects\\MyProject_perso.json'\n",
    "\n",
    "# use creds to create a client to interact with the Google Drive API\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(client_secret , scope)\n",
    "client = gspread.authorize(creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that will create a dataframe on Google Sheet with all the relevant information I need. \n",
    "\n",
    "def TwitterDf(t, keyword):\n",
    "    \n",
    "    #opening the Google Sheet\n",
    "    sh = client.open(\"NewsScrapy\")\n",
    "    sheet_3 = sh.get_worksheet(2) \n",
    "    #starting from the first row\n",
    "    sheet_3.resize(1) \n",
    "    \n",
    "    #creating a function to clean the tweet \n",
    "    def clean_tweet(tweet):\n",
    "         \n",
    "         return' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "     \n",
    "    #function that gives an estimation of the sentiment polarity of this tweet\n",
    "    def aylien_sentimentPolarity(text): \n",
    "        sentiment = client_aylien.Sentiment({'text': text})\n",
    "        return(sentiment['polarity'])\n",
    "    \n",
    "    #function that gives the degree of confidence of the sentiment polarity          \n",
    "    def aylien_sentimentConfidence(text): \n",
    "        sentiment = client_aylien.Sentiment({'text': text})\n",
    "        return(sentiment['polarity_confidence'])\n",
    " \n",
    "    #function to simplify the sentiment polarity into three categories \n",
    "    def analize_sentiment(tweet):\n",
    "             \n",
    "             analysis = TextBlob(clean_tweet(tweet))\n",
    "             if analysis.sentiment.polarity > 0:\n",
    "                 return 1\n",
    "             elif analysis.sentiment.polarity == 0:\n",
    "                 return 0\n",
    "             else:\n",
    "                 return -1\n",
    "    \n",
    "    #scraping the tweets from Twitter \n",
    "    try:\n",
    "        tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "        tso.set_result_type('recent')\n",
    "        tso.set_language('en')      \n",
    "        tso.set_keywords(keyword)   \n",
    "        tso.set_include_entities(True) \n",
    "        \n",
    "        #looping through the tweets collected and extracting the relevant data. \n",
    "        for tweet in ts.search_tweets_iterable(tso):  \n",
    "            \n",
    "            \n",
    "            if int(tweet['retweet_count']) > t: \n",
    "                  \n",
    "               report_sheet = [tweet['user']['screen_name'],\n",
    "                                    tweet['user']['name'],\n",
    "                                    tweet['user']['id'],\n",
    "                                    tweet['user']['followers_count'],\n",
    "                                    tweet['user']['location'],\n",
    "                               \n",
    "                               tweet['text'],\n",
    "                               tweet['entities']['hashtags'],\n",
    "                               tweet['retweet_count'],                          \n",
    "                               tweet['geo'],\n",
    "                               tweet['user']['time_zone'],\n",
    "                               tweet['created_at'], \n",
    "                               analize_sentiment(tweet['text']), \n",
    "                               aylien_sentimentPolarity(clean_tweet(tweet['text'])), \n",
    "                               aylien_sentimentConfidence(clean_tweet(tweet['text']))]\n",
    "               \n",
    "               #appending the rows to the Google Sheet\n",
    "               sheet_3.append_row(report_sheet)\n",
    "               \n",
    "               #verifying the number of tweets threated by displaying in my environment the Tweets ID \n",
    "               display(tweet['id']) \n",
    "               \n",
    "                \n",
    " \n",
    "    ## take care of all the errors if something goes wrong         \n",
    "    except TwitterSearchException as e: \n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
